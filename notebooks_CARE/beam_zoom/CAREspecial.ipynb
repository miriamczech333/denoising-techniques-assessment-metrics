{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import pickle\n",
    "\n",
    "import os, sys\n",
    "    \n",
    "from denoising_assessment_project.utils import metric_compute, stats_compute, stats_plot\n",
    "\n",
    "# parameters setup \n",
    "from denoising_assessment_project.global_vars import global_vars\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring variables from memory (GT images, indices)\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "09_10_00.tif\n",
      "09_10_01.tif\n",
      "09_10_02.tif\n",
      "09_10_03.tif\n",
      "09_10_04.tif\n",
      "09_10_05.tif\n",
      "09_10_06.tif\n",
      "09_10_07.tif\n",
      "09_10_08.tif\n",
      "09_10_09.tif\n",
      "09_10_10.tif\n",
      "09_10_11.tif\n",
      "09_10_12.tif\n",
      "09_10_13.tif\n",
      "09_10_14.tif\n",
      "09_10_15.tif\n",
      "09_10_16.tif\n",
      "09_10_17.tif\n",
      "09_10_18.tif\n",
      "09_10_19.tif\n",
      "09_10_20.tif\n",
      "09_10_21.tif\n",
      "09_10_22.tif\n",
      "09_10_23.tif\n",
      "09_10_24.tif\n",
      "09_10_25.tif\n",
      "09_10_26.tif\n",
      "09_10_27.tif\n",
      "09_10_28.tif\n",
      "09_10_29.tif\n",
      "09_10_30.tif\n",
      "09_10_31.tif\n",
      "09_10_32.tif\n",
      "09_10_33.tif\n",
      "09_10_34.tif\n",
      "09_10_35.tif\n",
      "09_10_36.tif\n",
      "09_10_37.tif\n",
      "09_10_38.tif\n",
      "09_10_39.tif\n",
      "09_10_40.tif\n",
      "09_10_41.tif\n",
      "09_10_42.tif\n",
      "09_10_43.tif\n",
      "09_10_44.tif\n",
      "09_10_45.tif\n",
      "09_10_46.tif\n",
      "09_10_47.tif\n",
      "09_10_48.tif\n",
      "09_10_49.tif\n"
     ]
    }
   ],
   "source": [
    "# collecting CARE-predicted images into a tensor, ensuring appropriate omage loactions relative to GT images\n",
    "import glob\n",
    "import fnmatch\n",
    "from tifffile import imread\n",
    "\n",
    "#path at which the predicted images are stored \n",
    "path = '/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/predictions/CAREspecial'\n",
    "\n",
    "predicted_images = sorted(os.listdir(path))\n",
    "predicted_images_len = len(predicted_images)\n",
    "\n",
    "print(predicted_images_len)\n",
    "predicted_images_tensor = torch.Tensor(50,256,256)\n",
    "\n",
    "for i in range(len(predicted_images)): \n",
    "    print(predicted_images[i])\n",
    "    predicted_images_tensor[i,:,:] = torch.from_numpy(imread(os.path.join(path, predicted_images[i])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_00.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_01.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_02.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_03.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_04.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_05.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_06.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_07.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_08.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_09.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_10.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_11.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_12.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_13.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_14.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_15.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_16.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_17.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_18.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_19.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_20.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_21.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_22.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_23.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_24.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_25.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_26.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_27.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_28.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_29.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_30.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_31.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_32.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_33.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_34.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_35.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_36.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_37.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_38.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_39.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_40.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_41.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_42.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_43.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_44.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_45.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_46.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_47.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_48.tif\n",
      "/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/09_10_49.tif\n"
     ]
    }
   ],
   "source": [
    "# opening GT images \n",
    "import glob\n",
    "import fnmatch\n",
    "from tifffile import imread\n",
    "\n",
    "#path at which the predicted images are stored \n",
    "path = '/Users/miriamczech/Desktop/denoising_assessment/tiffed_data_beam/EVAL_zoom/noised/'\n",
    "\n",
    "p = '09'\n",
    "b = '10'\n",
    "\n",
    "un_noised_images_list = sorted(glob.glob(os.path.join(path, '{}_{}_*.tif'.format(p,b))))\n",
    "\n",
    "un_noised_images = torch.Tensor(50,256,256)\n",
    "\n",
    "for i in range(len(un_noised_images_list)): \n",
    "    print(un_noised_images_list[i])\n",
    "    un_noised_images[i,:,:] = torch.from_numpy(imread(un_noised_images_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 50, 256, 256])\n",
      "torch.Size([1, 1, 50, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "un_noised_images = un_noised_images.unsqueeze(0).unsqueeze(0)\n",
    "predicted_images_tensor = predicted_images_tensor.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(un_noised_images.shape)\n",
    "print(predicted_images_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(un_noised_images_tensor): \n",
    "    un_noised_images_tensor_max = torch.amax(un_noised_images_tensor, dim=(3,4))\n",
    "    un_noised_images_tensor_min = torch.amin(un_noised_images_tensor, dim=(3,4))\n",
    "    un_noised_images_tensor_range = un_noised_images_tensor_max - un_noised_images_tensor_min\n",
    "    un_noised_images_tensor_threshold = un_noised_images_tensor_range/5 + un_noised_images_tensor_min\n",
    "    un_noised_images_tensor_threshold_expanded = un_noised_images_tensor_threshold.unsqueeze(3).unsqueeze(4).expand(un_noised_images_tensor.shape)\n",
    "    \n",
    "    image_masks = torch.where(un_noised_images_tensor <= un_noised_images_tensor_threshold_expanded, 0, 1)\n",
    "    return image_masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_masks = threshold(un_noised_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare un_noised masked and predicted masked - FOREGROUND\n",
    "\n",
    "# 0 - background \n",
    "# 1 - foreground \n",
    "\n",
    "# 1. un_noised masked \n",
    "un_noised_foreground = torch.where(image_masks == 0, 0, un_noised_images)\n",
    "un_noised_foreground[un_noised_foreground == 0] = np.nan\n",
    "\n",
    "# 1. predicted masked \n",
    "predicted_images_tensor_foreground  = torch.where(image_masks == 0, 0, predicted_images_tensor)\n",
    "predicted_images_tensor_foreground[predicted_images_tensor_foreground== 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare un_noised masked and predicted masked - BACKGROUND\n",
    "\n",
    "# 0 - background \n",
    "# 1 - foreground \n",
    "\n",
    "# 1. un_noised masked \n",
    "un_noised_background = torch.where(image_masks == 1, 1, un_noised_images)\n",
    "un_noised_background[un_noised_background == 1] = np.nan\n",
    "\n",
    "# 2. predicted masked \n",
    "predicted_images_tensor_background  = torch.where(image_masks == 1, 1, predicted_images_tensor)\n",
    "predicted_images_tensor_background[predicted_images_tensor_background== 1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1724, 0.1711, 0.1714, 0.1713, 0.1691, 0.1716, 0.1723, 0.1724,\n",
      "          0.1734, 0.1713, 0.1697, 0.1701, 0.1734, 0.1717, 0.1742, 0.1706,\n",
      "          0.1717, 0.1702, 0.1697, 0.1712, 0.1738, 0.1727, 0.1706, 0.1728,\n",
      "          0.1711, 0.1708, 0.1726, 0.1707, 0.1731, 0.1731, 0.1705, 0.1716,\n",
      "          0.1729, 0.1714, 0.1715, 0.1731, 0.1733, 0.1707, 0.1741, 0.1708,\n",
      "          0.1687, 0.1722, 0.1693, 0.1692, 0.1728, 0.1727, 0.1719, 0.1699,\n",
      "          0.1731, 0.1723]]])\n",
      "tensor(0.1716)\n",
      "tensor(0.0014)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -NRMSE-\n",
    "# 4a. Computing NRMSE for each GT-CARE_prediction image pair in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.NRMSE_compute(un_noised_images, predicted_images_tensor)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_NRMSE = torch.mean(metric_output)\n",
    "std_NRMSE = torch.std(metric_output)\n",
    "\n",
    "print(mean_NRMSE)\n",
    "print(std_NRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0961, 0.1018, 0.1041, 0.1004, 0.1069, 0.1090, 0.1133, 0.1070,\n",
      "          0.1090, 0.1064, 0.1043, 0.1090, 0.1135, 0.1023, 0.1054, 0.1113,\n",
      "          0.1085, 0.1090, 0.1022, 0.1079, 0.1023, 0.1151, 0.1080, 0.1059,\n",
      "          0.1084, 0.1061, 0.1040, 0.1088, 0.1078, 0.1083, 0.1067, 0.1000,\n",
      "          0.0997, 0.1092, 0.1100, 0.1034, 0.1106, 0.1013, 0.1038, 0.1135,\n",
      "          0.1048, 0.1033, 0.1067, 0.1065, 0.1048, 0.1067, 0.0944, 0.1116,\n",
      "          0.0947, 0.1045]]])\n",
      "tensor(0.1060)\n",
      "tensor(0.0045)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -NRMSE FOREGORUND-\n",
    "# 4a. Computing NRMSE for each GT-CARE_prediction image pair in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.NRMSE_compute(un_noised_foreground, predicted_images_tensor_foreground)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_NRMSE_f = torch.mean(metric_output)\n",
    "std_NRMSE_f = torch.std(metric_output)\n",
    "\n",
    "print(mean_NRMSE_f)\n",
    "print(std_NRMSE_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2083, 0.2235, 0.2386, 0.2309, 0.2438, 0.2593, 0.2603, 0.2508,\n",
      "          0.2483, 0.2402, 0.2321, 0.2397, 0.2484, 0.2367, 0.2348, 0.2571,\n",
      "          0.2545, 0.2374, 0.2527, 0.2333, 0.2254, 0.2560, 0.2418, 0.2344,\n",
      "          0.2393, 0.2427, 0.2300, 0.2465, 0.2551, 0.2544, 0.2538, 0.2300,\n",
      "          0.2176, 0.2474, 0.2424, 0.2491, 0.2463, 0.2337, 0.2349, 0.2647,\n",
      "          0.2385, 0.2303, 0.2413, 0.2360, 0.2290, 0.2389, 0.2103, 0.2518,\n",
      "          0.2020, 0.2322]]])\n",
      "tensor(0.2397)\n",
      "tensor(0.0133)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -NRMSE BACKGROUND-\n",
    "# 4a. Computing NRMSE for each GT-CARE_prediction image pair in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.NRMSE_compute(un_noised_background, predicted_images_tensor_background)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_NRMSE_b = torch.mean(metric_output)\n",
    "std_NRMSE_b = torch.std(metric_output)\n",
    "\n",
    "print(mean_NRMSE_b)\n",
    "print(std_NRMSE_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.2952, 6.2715, 6.1825, 6.2741, 6.2633, 6.2481, 6.1810, 6.2383,\n",
      "          6.1990, 6.2872, 6.2074, 6.2260, 6.1818, 6.2111, 6.1412, 6.2008,\n",
      "          6.2696, 6.1738, 6.2637, 6.1776, 6.2576, 6.0589, 6.2605, 6.2746,\n",
      "          6.2017, 6.2501, 6.2368, 6.2864, 6.1094, 6.2552, 6.2319, 6.3483,\n",
      "          6.2115, 6.1824, 6.2899, 6.1492, 6.2616, 6.2705, 6.2965, 6.1701,\n",
      "          6.3206, 6.2406, 6.2416, 6.2066, 6.1657, 6.2927, 6.2774, 6.2126,\n",
      "          6.2668, 6.2704]]])\n",
      "tensor(6.2318)\n",
      "tensor(0.0556)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -RMSE-\n",
    "# 4b. Computing RMSE for each image in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.RMSE_compute(un_noised_images, predicted_images_tensor)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_RMSE = torch.mean(metric_output)\n",
    "std_RMSE = torch.std(metric_output)\n",
    "\n",
    "print(mean_RMSE)\n",
    "print(std_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10.5349,  9.7761,  9.4703, 10.0547,  9.3467,  9.2148,  8.9534,\n",
      "           9.4478,  9.3019,  9.5053,  9.4171,  9.2196,  8.9705,  9.6599,\n",
      "           9.5061,  8.9819,  9.3833,  9.1428,  9.6629,  9.2855,  9.8929,\n",
      "           8.6536,  9.3403,  9.6949,  9.2600,  9.4180,  9.5423,  9.3849,\n",
      "           9.1820,  9.3254,  9.3454, 10.3172, 10.2135,  9.1829,  9.3599,\n",
      "           9.4772,  9.3858,  9.8459,  9.9818,  8.8153,  9.5673,  9.8161,\n",
      "           9.3454,  9.1681,  9.5679,  9.6596, 10.6187,  8.9785, 10.7436,\n",
      "           9.7108]]])\n",
      "tensor(9.5126)\n",
      "tensor(0.4392)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -RMSE FOREGORUND-\n",
    "# 4a. Computing NRMSE for each GT-CARE_prediction image pair in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.RMSE_compute(un_noised_foreground, predicted_images_tensor_foreground)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_RMSE_f = torch.mean(metric_output)\n",
    "std_RMSE_f = torch.std(metric_output)\n",
    "\n",
    "print(mean_RMSE_f)\n",
    "print(std_RMSE_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2083, 0.2235, 0.2386, 0.2309, 0.2438, 0.2593, 0.2603, 0.2508,\n",
      "          0.2483, 0.2402, 0.2321, 0.2397, 0.2484, 0.2367, 0.2348, 0.2571,\n",
      "          0.2545, 0.2374, 0.2527, 0.2333, 0.2254, 0.2560, 0.2418, 0.2344,\n",
      "          0.2393, 0.2427, 0.2300, 0.2465, 0.2551, 0.2544, 0.2538, 0.2300,\n",
      "          0.2176, 0.2474, 0.2424, 0.2491, 0.2463, 0.2337, 0.2349, 0.2647,\n",
      "          0.2385, 0.2303, 0.2413, 0.2360, 0.2290, 0.2389, 0.2103, 0.2518,\n",
      "          0.2020, 0.2322]]])\n",
      "tensor(0.2397)\n",
      "tensor(0.0133)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -RMSE BACKGROUND-\n",
    "# 4a. Computing NRMSE for each GT-CARE_prediction image pair in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.RMSE_compute(un_noised_background, predicted_images_tensor_background)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_RMSE_b = torch.mean(metric_output)\n",
    "std_RMSE_b = torch.std(metric_output)\n",
    "\n",
    "print(mean_RMSE_b)\n",
    "print(std_RMSE_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[35.4301, 33.4695, 32.3987, 32.7679, 31.6971, 31.2084, 30.3307,\n",
      "          32.2920, 30.9974, 32.4872, 32.8859, 31.5609, 31.6727, 33.7174,\n",
      "          32.9647, 30.6872, 31.9704, 31.8701, 32.4635, 31.9722, 33.7278,\n",
      "          30.0273, 31.6823, 33.0267, 31.7823, 32.6724, 33.5609, 31.3075,\n",
      "          31.5101, 31.9131, 31.5769, 33.2067, 34.1936, 31.4730, 31.6758,\n",
      "          32.2596, 31.6099, 33.3688, 33.2082, 30.5698, 32.0216, 33.0701,\n",
      "          31.8887, 31.8959, 32.8729, 32.4840, 35.4989, 31.1340, 35.8390,\n",
      "          33.0540]]])\n",
      "tensor(32.3791)\n",
      "tensor(1.2320)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -PSNR- \n",
    "# 4c. Computing PSNR for each image in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.PSNR_compute(un_noised_images, predicted_images_tensor)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_PSNR = torch.mean(metric_output)\n",
    "std_PSNR = torch.std(metric_output)\n",
    "\n",
    "print(mean_PSNR)\n",
    "print(std_PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[30.9577, 29.6137, 28.6948, 28.6715, 28.2200, 27.8336, 27.1122,\n",
      "          28.6868, 27.4724, 28.8970, 29.2659, 28.1509, 28.4387, 29.8813,\n",
      "          29.1698, 27.4687, 28.4681, 28.4596, 28.6979, 28.4324, 29.7495,\n",
      "          26.9314, 28.2071, 29.2475, 28.3002, 29.1110, 29.8671, 27.8269,\n",
      "          27.9713, 28.4447, 28.0574, 28.9885, 29.8741, 28.0365, 28.2233,\n",
      "          28.5025, 28.0941, 29.4498, 29.2061, 27.4710, 28.4210, 29.1358,\n",
      "          28.3827, 28.5075, 29.0562, 28.7616, 30.9330, 27.9353, 31.1569,\n",
      "          29.2548]]])\n",
      "tensor(28.7140)\n",
      "tensor(0.9094)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -PSNR FOREGORUND-\n",
    "# 4a. Computing NRMSE for each GT-CARE_prediction image pair in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.PSNR_compute(un_noised_foreground, predicted_images_tensor_foreground)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_PSNR_f = torch.mean(metric_output)\n",
    "std_PSNR_f = torch.std(metric_output)\n",
    "\n",
    "print(mean_PSNR_f)\n",
    "print(std_PSNR_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (57190) must match the size of tensor b (57266) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b37738697d8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 4a. Computing NRMSE for each GT-CARE_prediction image pair in the data collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# dims=[m, n, reps]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmetric_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_compute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPSNR_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mun_noised_background\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_images_tensor_background\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/denoising_assessment/denoising_assessment_project/utils/metric_compute.py\u001b[0m in \u001b[0;36mPSNR_compute\u001b[0;34m(un_noised_images, noised_images)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0marg_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mun_noised_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0marg_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoised_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0marg_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0marg_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mdim_i_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mdim_i_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_i_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_full_state_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_reduce_state_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36m_forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# calculate batch state and compute batch value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mbatch_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                             \u001b[0;34m\" device corresponds to the device of the input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                         ) from err\n\u001b[0;32m--> 400\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_on_cpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"Expected all tensors to be on\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchmetrics/image/psnr.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;34m\"\"\"Update state with predictions and targets.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0msum_squared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_psnr_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_range\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchmetrics/functional/image/psnr.py\u001b[0m in \u001b[0;36m_psnr_update\u001b[0;34m(preds, target, dim)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0msum_squared_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mn_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum_squared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (57190) must match the size of tensor b (57266) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# CARE: -PSNR BACKGROUND-\n",
    "# 4a. Computing NRMSE for each GT-CARE_prediction image pair in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.PSNR_compute(un_noised_background, predicted_images_tensor_background)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_PSNR_b = torch.mean(metric_output)\n",
    "std_PSNR_b = torch.std(metric_output)\n",
    "\n",
    "print(mean_PSNR_b)\n",
    "print(std_PSNR_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9980, 0.9954, 0.9947, 0.9861, 0.9935, 0.9957, 0.9931, 0.9984,\n",
      "          0.9824, 0.9976, 0.9971, 0.9911, 0.9994, 0.9994, 0.9990, 0.9951,\n",
      "          0.9994, 0.9948, 0.9975, 0.9905, 0.9971, 0.9871, 0.9934, 0.9988,\n",
      "          0.9911, 0.9993, 0.9993, 0.9921, 0.9957, 0.9990, 0.9973, 0.9915,\n",
      "          0.9983, 0.9941, 0.9975, 0.9943, 0.9968, 0.9985, 0.9972, 0.9988,\n",
      "          0.9918, 0.9939, 0.9941, 0.9925, 0.9923, 0.9974, 0.9977, 0.9967,\n",
      "          0.9987, 0.9971]]])\n",
      "tensor(0.9953)\n",
      "tensor(0.0037)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -SSIM- \n",
    "# 4c. Computing SSIM for each image in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.SSIM_compute(un_noised_images, predicted_images_tensor)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_SSIM = torch.mean(metric_output)\n",
    "std_SSIM = torch.std(metric_output)\n",
    "\n",
    "print(mean_SSIM)\n",
    "print(std_SSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9954, 0.9922, 0.9917, 0.9806, 0.9907, 0.9940, 0.9907, 0.9972,\n",
      "          0.9781, 0.9961, 0.9951, 0.9879, 0.9994, 0.9993, 0.9980, 0.9929,\n",
      "          0.9994, 0.9925, 0.9957, 0.9868, 0.9947, 0.9840, 0.9904, 0.9977,\n",
      "          0.9876, 0.9991, 0.9990, 0.9890, 0.9933, 0.9983, 0.9954, 0.9867,\n",
      "          0.9960, 0.9918, 0.9955, 0.9913, 0.9948, 0.9967, 0.9949, 0.9982,\n",
      "          0.9883, 0.9904, 0.9915, 0.9895, 0.9887, 0.9952, 0.9948, 0.9952,\n",
      "          0.9961, 0.9951]]])\n",
      "tensor(0.9931)\n",
      "tensor(0.0047)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -SSIM FOREGORUND-\n",
    "# 4a. Computing NRMSE for each GT-CARE_prediction image pair in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.SSIM_compute(un_noised_foreground, predicted_images_tensor_foreground)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_SSIM_f = torch.mean(metric_output)\n",
    "std_SSIM_f = torch.std(metric_output)\n",
    "\n",
    "print(mean_SSIM_f)\n",
    "print(std_SSIM_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9016, 0.8423, 0.8687, 0.8760, 0.8456, 0.8290, 0.8313, 0.8383,\n",
      "          0.8505, 0.8452, 0.8732, 0.8098, 0.8518, 0.8574, 0.8501, 0.8358,\n",
      "          0.8299, 0.8622, 0.8173, 0.8772, 0.8845, 0.7925, 0.8281, 0.8595,\n",
      "          0.8116, 0.8160, 0.8755, 0.8468, 0.8786, 0.8355, 0.8657, 0.8852,\n",
      "          0.8916, 0.8615, 0.8305, 0.8512, 0.8243, 0.8854, 0.8823, 0.8186,\n",
      "          0.8373, 0.8635, 0.8584, 0.8315, 0.8693, 0.8393, 0.8911, 0.8471,\n",
      "          0.9084, 0.8521]]])\n",
      "tensor(0.8523)\n",
      "tensor(0.0258)\n"
     ]
    }
   ],
   "source": [
    "# CARE: -SSIM BACKGROUND-\n",
    "# 4a. Computing NRMSE for each GT-CARE_prediction image pair in the data collection\n",
    "# dims=[m, n, reps]\n",
    "metric_output = metric_compute.SSIM_compute(un_noised_background, predicted_images_tensor_background)\n",
    "\n",
    "print(metric_output)\n",
    "\n",
    "mean_SSIM_b = torch.mean(metric_output)\n",
    "std_SSIM_b = torch.std(metric_output)\n",
    "\n",
    "print(mean_SSIM_b)\n",
    "print(std_SSIM_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
